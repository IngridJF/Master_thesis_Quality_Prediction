{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284f9dc-6f1e-4e6a-8339-473d582e14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing XGBoost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edc0f2-295a-4807-bb53-4adad5541dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the needed libraries\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562d789-b9fa-4e70-8d9e-0ade6122efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx')\n",
    "\n",
    "#dropping unwanted columns from the df\n",
    "df2=df.drop(columns = [\"Column 1\", \"Column 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07758532-bd10-424f-996b-4fdef3534632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on ID\n",
    "df2[\"ID\"] = df2[\"ID\"].astype(str)  #Making sure its a string\n",
    "df_train = df2[df2[\"ID\"].isin([\"ID 1\", \"ID 2\", \"ID 3\"])]\n",
    "df_unseen = df2[df2[\"ID\"] == \"ID 4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90c4cc-48e2-4e99-9299-ee715b9b865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining which columns to keep av model input\n",
    "X = df_train.drop(columns = ['Column 1', 'Column 2', 'Column 3'])\n",
    "\n",
    "#defining which columns are the model outputs\n",
    "y = df_train['Column 4']\n",
    "\n",
    "#Setting up k-fold cross validation\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b5721-a920-43ce-985d-6c08d48a03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing XGBoost classifier for multi-class classification\n",
    "#\"multi:softmax\": outputs the class with the highest predicted probability\n",
    "#\"mlogloss\": log loss metric for multi-class evaluation\n",
    "XGB = xgb.XGBClassifier(objective=\"multi:softmax\", eval_metric=\"mlogloss\")\n",
    "\n",
    "#Useing the first fold from K-Fold cross-validation\n",
    "train_index, test_index = next(k_fold.split(X, y))\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#XGBoost requires class labels to be zero-indexed (starting from 0)\n",
    "#Shifting all values to start from 0 by subtracting the minimum class value\n",
    "y_train_fixed = y_train - y_train.min()\n",
    "y_test_fixed = y_test - y_test.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd75208-d687-4312-987c-0b022787bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a StandardScaler to normalize feature values\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fitting the scaler on the training data and transforming it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#Transforming the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc08f01-72ed-4213-9912-1f6526a72a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameter grid for XGBoost classification\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],     # Number of boosting rounds (trees)\n",
    "    'max_depth': [3, 6, 9, 12],          # Maximum depth of each tree (controls model complexity)\n",
    "    'learning_rate': [0.04, 0.05, 0.06], # Step size shrinkage (lower = more robust, slower learning)\n",
    "    'subsample': [0.6, 0.7, 0.8],        # Fraction of training samples to use per boosting round (controls overfitting)\n",
    "    'colsample_bytree': [0.5,0.6,0.7],   # Fraction of features used per tree (helps generalization)\n",
    "    'reg_alpha': [0.1, 1],               # L1 regularization term (sparse weights)\n",
    "    'reg_lambda': [1, 10],               # L2 regularization term (shrinks weights)\n",
    "}\n",
    "\n",
    "#Setting up GridSearchCV to tune hyperparameters\n",
    "CV_XGB = GridSearchCV(\n",
    "    XGB,                             \n",
    "    param_grid,                       \n",
    "    cv=5,                             \n",
    "    scoring='f1_weighted',           \n",
    "    n_jobs=-1,                        \n",
    "    verbose=1                        \n",
    ")\n",
    "\n",
    "#Fitting GridSearchCV using the scaled training data and zero-based target labels\n",
    "CV_XGB.fit(X_train_scaled, y_train_fixed)\n",
    "\n",
    "#Printing the best combination of hyperparameters found\n",
    "print('Best Parameters: ', CV_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8881ba-ad29-43f0-b4b5-a43771f808b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the best XGBoost model from GridSearchCV\n",
    "XGB_best = CV_XGB.best_estimator_\n",
    "\n",
    "#Storing the best hyperparameters\n",
    "best_params = CV_XGB.best_params_\n",
    "\n",
    "#Making predictions on the training set using the best model\n",
    "pred_train = XGB_best.predict(X_train_scaled)\n",
    "\n",
    "#Making predictions on the test set using the same model\n",
    "pred_test = XGB_best.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59434aca-c39d-4fb6-9ee5-ebfa09a025c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating model performance on the training data\n",
    "acc_train = accuracy_score(y_train_fixed, pred_train)\n",
    "precision_train = precision_score(y_train_fixed, pred_train, average='weighted')\n",
    "recall_train = recall_score(y_train_fixed, pred_train, average='weighted')\n",
    "f1_train = f1_score(y_train_fixed, pred_train, average='weighted')\n",
    "\n",
    "#Printing training performance metrics\n",
    "print(f'Train Accuracy: {acc_train:.4f}')\n",
    "print(f'Train Precision: {precision_train:.4f}')\n",
    "print(f'Train Recall: {recall_train:.4f}')\n",
    "print(f'Train F1 Score: {f1_train:.4f}')\n",
    "\n",
    "#Generating confusion matrix to visualize prediction errors and correct classifications\n",
    "cm = confusion_matrix(y_train_fixed, pred_train)\n",
    "\n",
    "#Creating class labels for the heatmap\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_train_fixed)]\n",
    "\n",
    "#Plotting the confusion matrix as a heatmap\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08422fd1-6812-4d19-b59a-ef84ff704c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating model performance on the test data\n",
    "acc_test = accuracy_score(y_test_fixed, pred_test)\n",
    "precision_test = precision_score(y_test_fixed, pred_test, average='weighted')\n",
    "recall_test = recall_score(y_test_fixed, pred_test, average='weighted')\n",
    "f1_test = f1_score(y_test_fixed, pred_test, average='weighted')\n",
    "\n",
    "#Printing test performance metrics\n",
    "print(f'Test Accuracy: {acc_test:.4f}')\n",
    "print(f'Test Precision: {precision_test:.4f}')\n",
    "print(f'Test Recall: {recall_test:.4f}')\n",
    "print(f'Test F1 Score: {f1_test:.4f}')\n",
    "\n",
    "#Generating confusion matrix for the test set\n",
    "cm = confusion_matrix(y_test_fixed, pred_test)\n",
    "\n",
    "#Defining class labels for the confusion matrix\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_test_fixed)]\n",
    "\n",
    "#Plotting the confusion matrix as a heatmap\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de59ed-1686-429e-a882-d61f74e967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining inputs and outputs for unseen dataset\n",
    "X_unseen = df_unseen.drop(columns = ['Column 1', 'Column 2', 'Column 3')\n",
    "y_unseen = df_unseen['Column 4']\n",
    "\n",
    "#Changing unseen labels to start from 0, like the training labels used in XGBoost\n",
    "y_unseen_fixed = y_unseen - y_train.min()\n",
    "\n",
    "#Scaling unseen features using the same scaler fitted on training data\n",
    "X_unseen_scaled = scaler.transform(X_unseen)\n",
    "\n",
    "#Making predictions on the unseen data using the tuned XGBoost model\n",
    "y_pred_unseen = XGB_best.predict(X_unseen_scaled)\n",
    "\n",
    "#Computing evaluation metrics for the unseen set\n",
    "acc_unseen = accuracy_score(y_unseen_fixed, y_pred_unseen)                       \n",
    "precision_unseen = precision_score(y_unseen_fixed, y_pred_unseen, average='weighted', zero_division=0)\n",
    "recall_unseen = recall_score(y_unseen_fixed, y_pred_unseen, average='weighted', zero_division=0)\n",
    "f1_unseen = f1_score(y_unseen_fixed, y_pred_unseen, average='weighted', zero_division=0)\n",
    "\n",
    "#Printing unseen performance metrics\n",
    "print(f'Unseen Accuracy: {acc_unseen:.4f}')\n",
    "print(f'Unseen Precision: {precision_unseen:.4f}')\n",
    "print(f'Unseen Recall: {recall_unseen:.4f}')\n",
    "print(f'Unseen F1 Score: {f1_unseen:.4f}')\n",
    "\n",
    "#Generating and visualizing confusion matrix\n",
    "cm_unseen = confusion_matrix(y_unseen_fixed, y_pred_unseen)\n",
    "\n",
    "#making sure class labels in the heatmap match both predicted and actual labels\n",
    "all_labels = np.unique(np.concatenate((y_unseen_fixed, y_pred_unseen)))\n",
    "class_labels = [f\"Class {i}\" for i in all_labels]\n",
    "\n",
    "#Plotting confusion matrix\n",
    "sns.heatmap(cm_unseen, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbe8ed-7541-4d24-8420-200cba34dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, accuracy_train, precision_train, recall_train, f1_train,\n",
    "                accuracy_test, precision_test, recall_test, f1_test,\n",
    "                accuracy_unseen, precision_unseen, recall_unseen, f1_unseen,\n",
    "                filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs classification model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    #Creating DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params,\n",
    "        \"Accuracy_Train\": accuracy_train,\n",
    "        \"Precision_Train\": precision_train,\n",
    "        \"Recall_Train\": recall_train,\n",
    "        \"F1_Train\": f1_train,\n",
    "        \"Accuracy_Test\": accuracy_test,\n",
    "        \"Precision_Test\": precision_test,\n",
    "        \"Recall_Test\": recall_test,\n",
    "        \"F1_Test\": f1_test,\n",
    "        \"Accuracy_Unseen\": accuracy_unseen,\n",
    "        \"Precision_Unseen\": precision_unseen,\n",
    "        \"Recall_Unseen\": recall_unseen,\n",
    "        \"F1_Unseen\": f1_unseen\n",
    "    }])\n",
    "\n",
    "    # Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it properly before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Read existing sheet\n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result  #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "        except PermissionError:\n",
    "            #Printing error to warn user\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "            \n",
    "    #Printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train Accuracy={accuracy_train:.4f}, Test Accuracy={accuracy_test:.4f}, Unseen Accuracy={accuracy_unseen:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952e1d7-a7f6-4d20-9cf5-a17ade2d9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Using the function to log results from XGB regression modellog_results(\n",
    "    model_name=\"XGB_class\",\n",
    "    params=best_params,\n",
    "\n",
    "    accuracy_train=acc_train,\n",
    "    precision_train=precision_train,\n",
    "    recall_train=recall_train,\n",
    "    f1_train=f1_train,\n",
    "\n",
    "    accuracy_test=acc_test,\n",
    "    precision_test=precision_test,\n",
    "    recall_test=recall_test,\n",
    "    f1_test=f1_test,\n",
    "\n",
    "    accuracy_unseen=acc_unseen,\n",
    "    precision_unseen=precision_unseen,\n",
    "    recall_unseen=recall_unseen,\n",
    "    f1_unseen=f1_unseen\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99928f5a-0153-4127-b736-86033fc2b267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57111fd-8068-4678-b8d5-38294d25263e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
