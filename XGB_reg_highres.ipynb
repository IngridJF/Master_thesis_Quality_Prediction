{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bc4ba-77a9-4474-809a-1705dd285f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing XGBoost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b7fb0-cb4e-4617-86c6-2dcfad6ea78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24ef73-c26f-4c07-9de7-496ec8d146f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx')\n",
    "\n",
    "#dropping unwanted columns from the df\n",
    "df2=df.drop(columns = [\"Column 1\", \"Column 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b2108-a099-4ba0-b239-c2005e8cdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on ID\n",
    "df2[\"ID\"] = df2[\"ID\"].astype(str)  #Making sure its a string\n",
    "df_train = df2[df2[\"ID\"].isin([\"ID 1\", \"ID 2\", \"ID 3\"])]\n",
    "df_unseen = df2[df2[\"ID\"] == \"ID 4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765f73e-f87c-4265-b502-9f3a14c25100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining which columns to keep av model input\n",
    "X = df_train.drop(columns = ['Column 1', 'Column 2', 'Column 3'])\n",
    "\n",
    "#defining which columns are the model outputs\n",
    "y = df_train['Column 4']\n",
    "\n",
    "#defining inputs and outputs for unseen dataset\n",
    "X_unseen = df_unseen.drop(columns = ['Column 1', 'Column 2', 'Column 3')\n",
    "y_unseen = df_unseen['Column 4']\n",
    "\n",
    "#Setting up k-fold cross validation\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7280f7-e6aa-4ab6-ab24-c39a4bc7a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using K-Fold cross-validation to split data into training and test sets\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)\n",
    "\n",
    "#Defining a pipeline that includes feature scaling and an XGBoost regressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),                      \n",
    "    ('xg', xg.XGBRegressor(n_estimators=40,                \n",
    "                           random_state=40))           \n",
    "])\n",
    "\n",
    "#Creating lists to store evaluation metrics for each fold\n",
    "mae_scores_train, mse_scores_train, r2_scores_train = [], [], []\n",
    "mae_scores_test, mse_scores_test, r2_scores_test = [], [], []\n",
    "\n",
    "#Looping over the K-Fold splits\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    #Creating training and test sets for the fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #Skipping the fold if the test set contains only one unique target value\n",
    "    if len(np.unique(y_test)) == 1:\n",
    "        print(\"Skipping this fold due to only one class in test set.\")\n",
    "        continue\n",
    "\n",
    "    #Training the pipeline (scaling + XGBoost) on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    #Making predictions for both training and test sets\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    #Evaluating training performance\n",
    "    mae_train = mean_absolute_error(y_train, pred_train)\n",
    "    mse_train = mean_squared_error(y_train, pred_train)\n",
    "    r2_train = r2_score(y_train, pred_train)\n",
    "\n",
    "    #Evaluating test performance\n",
    "    mae_test = mean_absolute_error(y_test, pred_test)\n",
    "    mse_test = mean_squared_error(y_test, pred_test)\n",
    "    r2_test = r2_score(y_test, pred_test)\n",
    "\n",
    "    #Storing metrics for the fold\n",
    "    mae_scores_train.append(mae_train)\n",
    "    mse_scores_train.append(mse_train)\n",
    "    r2_scores_train.append(r2_train)\n",
    "\n",
    "    mae_scores_test.append(mae_test)\n",
    "    mse_scores_test.append(mse_test)\n",
    "    r2_scores_test.append(r2_test)\n",
    "\n",
    "    #Printing performance for the fold\n",
    "    print(f\"Train -> MAE: {mae_train:.6f}, MSE: {mse_train:.6f}, R²: {r2_train:.6f}\")\n",
    "    print(f\"Test  -> MAE: {mae_test:.6f}, MSE: {mse_test:.6f}, R²: {r2_test:.6f}\\n\")\n",
    "\n",
    "#Printing average performance across all folds\n",
    "print(\"\\nAverage results for all folds:\")\n",
    "print(f\"Train Mean MAE: {np.mean(mae_scores_train):.6f}, Test Mean MAE: {np.mean(mae_scores_test):.6f}\")\n",
    "print(f\"Train Mean MSE: {np.mean(mse_scores_train):.6f}, Test Mean MSE: {np.mean(mse_scores_test):.6f}\")\n",
    "print(f\"Train Mean R²: {np.mean(r2_scores_train):.6f}, Test Mean R²: {np.mean(r2_scores_test):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c46bf-abbe-46c5-92b6-1fda1cea7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a pipeline that includes feature scaling and an XGBoost regressor\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),            \n",
    "    ('xg', xg.XGBRegressor(random_state=66)) \n",
    "])\n",
    "\n",
    "#Defining the hyperparameter grid for tuning the XGBoost model\n",
    "parameters_grid = {\n",
    "    'xg__n_estimators': [300, 500, 750],      #Number of boosting rounds (trees)\n",
    "    'xg__max_depth': [3,4,5],                 #Maximum depth of each tree\n",
    "    'xg__learning_rate': [0.05, 0.08, 0.1],   #Step size shrinkage used in updates\n",
    "    'xg__subsample': [0.6, 0.8, 1],           #Fraction of training samples to use per tree\n",
    "    'xg__colsample_bytree': [0.3, 0.4, 0.5],  #Fraction of features used in each tree\n",
    "    'xg__gamma': [1, 2, 4, 5],                #Minimum loss reduction required to make a further split\n",
    "    'xg__reg_alpha': [3,6, 9],                #L1 regularization term on weights\n",
    "    'xg__reg_lambda': [10,15,20,27],          #L2 regularization term on weights\n",
    "    'xg__min_child_weight': [5,10,15]         #Minimum sum of instance weight (hessian) needed in a child\n",
    "}\n",
    "\n",
    "#Running GridSearchCV to perform hyperparameter tuning\n",
    "CV_XGB = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=parameters_grid,\n",
    "    cv=k_fold,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1                                \n",
    ")\n",
    "\n",
    "#Fitting the model on the training data\n",
    "CV_XGB.fit(X_train, y_train)\n",
    "\n",
    "#Printing the best combination of hyperparameters found during the search\n",
    "print('Best parameters: ', CV_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f55fec-d9b9-41eb-9450-e220bfe849ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the 'xg__' prefix from best_params\n",
    "best_params = {key.replace(\"xg__\", \"\"): value for key, value in CV_XGB.best_params_.items()}\n",
    "\n",
    "#Getting the best pipeline\n",
    "best_pipeline = CV_XGB.best_estimator_\n",
    "\n",
    "#Making predictions using the full pipeline on training and test sets\n",
    "y_train_pred = best_pipeline.predict(X_train)\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "#Evaluating performance on the training set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "#Evaluating performance on the test set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#Printing model performance metrics\n",
    "print(f\"Train Mean Squared Error: {mse_train:.4f}\")\n",
    "print(f\"Train R-squared: {r2_train:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse_test:.4f}\")\n",
    "print(f\"Test R-squared: {r2_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00de1c0-212f-4d8d-a7b2-f9ee69853bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on the unseen dataset using the best pipeline\n",
    "y_unseen_pred = best_pipeline.predict(X_unseen)\n",
    "\n",
    "#Calculating evaluation metrics on the unseen dataset\n",
    "mse_unseen = mean_squared_error(y_unseen, y_unseen_pred)  \n",
    "r2_unseen = r2_score(y_unseen, y_unseen_pred)             \n",
    "\n",
    "#Printing the metrics for unseen data\n",
    "print(f'Unseen Mean Squared Error: {mse_unseen:.4f}')\n",
    "print(f'Unseen R-squared: {r2_unseen:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca34f2-2a78-4c82-8019-7f6c1958f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, mse_train, r2_train, mse_test, r2_test, mse_unseen, r2_unseen, filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    #Creating DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params, \n",
    "        \"MSE_Train\": mse_train,\n",
    "        \"R2_Train\": r2_train,\n",
    "        \"MSE_Test\": mse_test,\n",
    "        \"R2_Test\": r2_test,\n",
    "        \"MSE_Unseen\": mse_unseen,\n",
    "        \"R2_Unseen\": r2_unseen\n",
    "    }])\n",
    "\n",
    "    #Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it properly before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Reading existing sheet \n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result  #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results, ensuring correct appending\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "        except PermissionError:\n",
    "            #Printing error to warn user\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "    \n",
    "    #Printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train MSE={mse_train:.4f}, Test MSE={mse_test:.4f}, Unseen MSE={mse_unseen:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df27b6-c83c-4b35-9b70-dfb86a46a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function to log results from XGB regression model\n",
    "log_results(\n",
    "    model_name=\"XGBoost\",\n",
    "    params=best_params,\n",
    "    mse_train=mse_train, \n",
    "    r2_train=r2_train,\n",
    "    mse_test=mse_test,\n",
    "    r2_test=r2_test,\n",
    "    mse_unseen=mse_unseen,\n",
    "    r2_unseen=r2_unseen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d08e8-c2e5-41f3-9add-b1ef5c224a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5a14d-ba7e-4e6c-bd2d-a378548a84f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
