{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d7056-8aaa-4edf-af54-a33ed1c954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753286d-75e5-4afd-80ac-fe6a4d0e2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx')\n",
    "\n",
    "#dropping unwanted columns from the df\n",
    "df2=df.drop(columns = [\"Column 1\", \"Column 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef8a86-6e95-439c-abb0-9782f1fec7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on ID\n",
    "df2[\"ID\"] = df2[\"ID\"].astype(str)  #Making sure its a string\n",
    "df_train = df2[df2[\"ID\"].isin([\"ID 1\", \"ID 2\", \"ID 3\"])]\n",
    "df_unseen = df2[df2[\"ID\"] == \"ID 4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d1551-9956-4bef-b811-a044aa590991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining which columns to keep av model input\n",
    "X = df_train.drop(columns = ['Column 1', 'Column 2', 'Column 3'])\n",
    "\n",
    "#defining which columns are the model outputs\n",
    "y = df_train['Column 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852144a-b6d7-4ad6-b7c1-748ed2a56913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up K-Fold cross-validation\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)\n",
    "\n",
    "#Displaying number of splits\n",
    "k_fold.get_n_splits(X, y)\n",
    "\n",
    "#Initializing the Random Forest Classifier\n",
    "RF = RandomForestClassifier(\n",
    "    random_state=66,        \n",
    "    class_weight='balanced' #Handle imbalance between classes\n",
    ")\n",
    "\n",
    "#Creating lists to store evaluation metrics from each fold\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "#Looping over each train-test split in the KFold cross-validation\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    #Splitting data into training and test sets for this fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #Skipping the fold if the test set contains only one class\n",
    "    if len(np.unique(y_test)) == 1:\n",
    "        print(\"Skipping this fold due to only one class in test set.\")\n",
    "        continue\n",
    "\n",
    "    #Initializing a new scaler and applying it to the training and test data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)    #Only fitted on train data\n",
    "    X_test_scaled = scaler.transform(X_test)          #Transforming the test data on the same scaler\n",
    "\n",
    "    #Training the Random Forest model on the scaled training data\n",
    "    RF.fit(X_train_scaled, y_train)\n",
    "\n",
    "    #Predicting the target values for the test set\n",
    "    pred_test = RF.predict(X_test_scaled)\n",
    "\n",
    "    #Calculating and storing classification metrics for this fold\n",
    "    accuracies.append(accuracy_score(y_test, pred_test))\n",
    "    precisions.append(precision_score(y_test, pred_test, average='weighted', zero_division=0.0))\n",
    "    recalls.append(recall_score(y_test, pred_test, average='weighted', zero_division=0.0))\n",
    "    f1_scores.append(f1_score(y_test, pred_test, average='weighted'))\n",
    "\n",
    "#Printing the average performance metrics across all valid folds\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f} Â± {np.std(accuracies):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961520f6-7c9d-43de-9f9f-f451ae12930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using GridSearchCV to find the best hyperparameters for the Random Forest model\n",
    "\n",
    "#Defining the hyperparameter grid to search over\n",
    "parameters_grid = {\n",
    "    'n_estimators': [75,100,150,300],         #Number of trees in the forest\n",
    "    'max_features': ['sqrt', 'log2'],         #Strategy to select the number of features at each split \n",
    "    'max_depth': [3,6,9,12,15],               #Maximum depth of each tree\n",
    "    'min_samples_split': [5,10,15],           #Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1,3,5,7,9],          #Minimum number of samples required to be at a leaf node\n",
    "    'criterion': ['entropy', 'gini'],         #Function to measure the quality of a split ('entropy' uses information gain)\n",
    "    'random_state': [66]                      #Fixed random seed for reproducibility\n",
    "}\n",
    "\n",
    "#Setting up the GridSearchCV\n",
    "CV_RF = GridSearchCV(\n",
    "    estimator=RF,                    \n",
    "    param_grid=parameters_grid,      \n",
    "    cv=k_fold,                        \n",
    "    scoring='accuracy'               \n",
    ")\n",
    "\n",
    "#Fitting the model to the scaled training data using the defined grid\n",
    "CV_RF.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Printing the best combination of hyperparameters found during grid search\n",
    "print('Best parameters: ', CV_RF.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958f6d8-bfc0-434b-b2c1-92f88c97792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the best hyperparameters from GridSearchCV\n",
    "best_params = CV_RF.best_params_\n",
    "\n",
    "#Initializing the Random Forest model using the best hyperparameters with class_weight = 'balanced' to handle class imbalance\n",
    "RF_best = RandomForestClassifier(**best_params, class_weight='balanced')\n",
    "\n",
    "#Fitting the best model to the training data\n",
    "RF_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predicting target values on the test set\n",
    "pred_test = RF_best.predict(X_test_scaled)\n",
    "\n",
    "#Predicting target values on the training set\n",
    "pred_train = RF_best.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c08b8-ed5d-4d4d-adeb-a696943249d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating performance metrics for classification models\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "precision_train = precision_score(y_train, pred_train, average='weighted')\n",
    "recall_train = recall_score(y_train, pred_train, average='weighted')\n",
    "f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "\n",
    "#Printing the evaluation metrics\n",
    "print(f'Accuracy: {acc_train:.4f}')\n",
    "print(f'Precision: {precision_train:.4f}')\n",
    "print(f'Recall: {recall_train:.4f}')\n",
    "print(f'F1 Score: {f1_train:.4f}')\n",
    "\n",
    "\n",
    "#Creating a confusion matrix from true and predicted labels\n",
    "cm = confusion_matrix(y_train, pred_train)\n",
    "\n",
    "#Defining class labels\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_train)]\n",
    "\n",
    "#Plotting the confusion matrix as a heatmap\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd2ac2-e87e-4617-a34e-5ca13247d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating performance metrics based on the test data\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "precision_test = precision_score(y_test, pred_test, average='weighted')\n",
    "recall_test = recall_score(y_test, pred_test, average='weighted', zero_division=0.0) #zero_division=0.0 handles cases where a class is not predicted at all\n",
    "f1_test = f1_score(y_test, pred_test, average='weighted')\n",
    "\n",
    "#Printing all test metrics\n",
    "print(f'Accuracy: {acc_test:.4f}')\n",
    "print(f'Precision: {precision_test:.4f}')\n",
    "print(f'Recall: {recall_test:.4f}')\n",
    "print(f'F1 Score: {f1_test:.4f}')\n",
    "\n",
    "#Ensuring consistency in the confusion matrix even if some classes are missing in y_test\n",
    "all_labels = np.unique(y)\n",
    "\n",
    "#Computing the confusion matrix using all possible class labels\n",
    "cm = confusion_matrix(y_test, pred_test, labels=all_labels)\n",
    "\n",
    "#Plotting the confusion matrix as a heatmap for easier visual interpretation\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=all_labels, yticklabels=all_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c587b-3d9b-4795-ab1c-e2f567c032bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining inputs and outputs for unseen dataset\n",
    "X_unseen = df_unseen.drop(columns = ['Column 1', 'Column 2', 'Column 3')\n",
    "y_unseen = df_unseen['Column 4']                               \n",
    "\n",
    "#Scaling the unseen data using the same scaler fitted on the training data\n",
    "X_unseen_scaled = scaler.transform(X_unseen)                      \n",
    "\n",
    "#Making predictions on the unseen data using the trained and tuned Random Forest model\n",
    "y_pred_unseen = RF_best.predict(X_unseen_scaled)\n",
    "\n",
    "#Evaluating model performance on the unseen data\n",
    "acc_unseen = accuracy_score(y_unseen, y_pred_unseen)                    \n",
    "precision_unseen = precision_score(y_unseen, y_pred_unseen, average='weighted', zero_division=0) \n",
    "recall_unseen = recall_score(y_unseen, y_pred_unseen, average='weighted', zero_division=0)\n",
    "f1_unseen = f1_score(y_unseen, y_pred_unseen, average='weighted', zero_division=0)\n",
    "\n",
    "#Printing evaluation metrics\n",
    "print(f'Accuracy: {acc_unseen:.4f}')\n",
    "print(f'Precision: {precision_unseen:.4f}')\n",
    "print(f'Recall: {recall_unseen:.4f}')\n",
    "print(f'F1 Score: {f1_unseen:.4f}')\n",
    "\n",
    "\n",
    "#Generating the confusion matrix\n",
    "cm_unseen = confusion_matrix(y_unseen, y_pred_unseen)\n",
    "\n",
    "#Createing readable class labels based on training classes\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_train)]\n",
    "\n",
    "#Plotting confusion matrix as heatmap\n",
    "sns.heatmap(cm_unseen, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b71e2-6201-47a2-b6a2-e4e94c56a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, accuracy_train, precision_train, recall_train, f1_train,\n",
    "                accuracy_test, precision_test, recall_test, f1_test,\n",
    "                accuracy_unseen, precision_unseen, recall_unseen, f1_unseen,\n",
    "                filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs classification model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    #Creating DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params,  # Store hyperparameters\n",
    "        \"Accuracy_Train\": accuracy_train,\n",
    "        \"Precision_Train\": precision_train,\n",
    "        \"Recall_Train\": recall_train,\n",
    "        \"F1_Train\": f1_train,\n",
    "        \"Accuracy_Test\": accuracy_test,\n",
    "        \"Precision_Test\": precision_test,\n",
    "        \"Recall_Test\": recall_test,\n",
    "        \"F1_Test\": f1_test,\n",
    "        \"Accuracy_Unseen\": accuracy_unseen,\n",
    "        \"Precision_Unseen\": precision_unseen,\n",
    "        \"Recall_Unseen\": recall_unseen,\n",
    "        \"F1_Unseen\": f1_unseen\n",
    "    }])\n",
    "\n",
    "    #Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it properly before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Reading existing sheet\n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result  #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results, ensuring correct appending\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "        except PermissionError:\n",
    "            #Printing error to warn user\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "            \n",
    "    #Printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train Accuracy={accuracy_train:.4f}, Test Accuracy={accuracy_test:.4f}, Unseen Accuracy={accuracy_unseen:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a1351-4ed6-42d7-b905-0dab12ea73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function to log results from RF classification model\n",
    "log_results(\n",
    "    model_name=\"RF_class\",\n",
    "    params=best_params,\n",
    "\n",
    "    accuracy_train=acc_train,\n",
    "    precision_train=precision_train,\n",
    "    recall_train=recall_train,\n",
    "    f1_train=f1_train,\n",
    "\n",
    "    accuracy_test=acc_test,\n",
    "    precision_test=precision_test,\n",
    "    recall_test=recall_test,\n",
    "    f1_test=f1_test,\n",
    "\n",
    "    accuracy_unseen=acc_unseen,\n",
    "    precision_unseen=precision_unseen,\n",
    "    recall_unseen=recall_unseen,\n",
    "    f1_unseen=f1_unseen\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
