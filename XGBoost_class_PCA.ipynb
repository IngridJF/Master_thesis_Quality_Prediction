{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284f9dc-6f1e-4e6a-8339-473d582e14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing the XGBoost package\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edc0f2-295a-4807-bb53-4adad5541dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562d789-b9fa-4e70-8d9e-0ade6122efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx')\n",
    "\n",
    "#dropping unwanted columns from the df\n",
    "df2=df.drop(columns = [\"Column 1\", \"Column 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07758532-bd10-424f-996b-4fdef3534632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on ID\n",
    "df2[\"ID\"] = df2[\"ID\"].astype(str)  #Making sure its a string\n",
    "df_train = df2[df2[\"ID\"].isin([\"ID 1\", \"ID 2\", \"ID 3\"])]\n",
    "df_unseen = df2[df2[\"ID\"] == \"ID 4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90c4cc-48e2-4e99-9299-ee715b9b865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing PCA input by defining the columns to include in the transformation\n",
    "columns = ['Column 1', 'Column 2', 'Column 3', 'Column 4', 'Column 5']\n",
    "\n",
    "#Extracting and copying the selected columns from the training dataframe\n",
    "data_train = df_train[columns].copy()\n",
    "\n",
    "#Encoding 'ID' \n",
    "data_train['ID'] = data_train['ID'].astype('Category_column').cat.codes\n",
    "\n",
    "#Initializing a standard scaler to normalize the data\n",
    "scaler_pca = StandardScaler()\n",
    "\n",
    "#Fitting the scaler on training data and apply the transformation\n",
    "data_train_scaled = scaler_pca.fit_transform(data_train)\n",
    "\n",
    "#Initializing PCA to reduce dimensionality to 4 PCs\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "#Fitting PCA on the scaled training data and transforming it\n",
    "pca_train = pca.fit_transform(data_train_scaled)\n",
    "\n",
    "#Adding the first 4 principal components (PC1 to PC4) as new columns to df_train\n",
    "for i in range(4):\n",
    "    df_train.loc[:, f'PC{i+1}'] = pca_train[:, i]\n",
    "\n",
    "\n",
    "#Applying the same PCA transformation to the unseen dataset \n",
    "\n",
    "#Extracting and copying the same columns from the unseen dataset\n",
    "data_unseen = df_unseen[columns].copy()\n",
    "\n",
    "#Encode 'ID' \n",
    "data_unseen['ID'] = data_unseen['ID'].astype('Category_column').cat.codes\n",
    "\n",
    "#Applying the same scaling \n",
    "data_unseen_scaled = scaler_pca.transform(data_unseen)\n",
    "\n",
    "#Applying the same PCA transformation\n",
    "pca_unseen = pca.transform(data_unseen_scaled)\n",
    "\n",
    "#Adding PC1 to PC4 as new columns to df_unseen\n",
    "for i in range(4):\n",
    "    df_unseen.loc[:, f'PC{i+1}'] = pca_unseen[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67853e7-83c6-44b9-b4f3-6d7efa41f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing inputs and target values\n",
    "X = df_train[['PC1', 'PC2', 'PC3', 'PC4']]\n",
    "y = df_train['Category_column']\n",
    "\n",
    "X_unseen = df_unseen[['PC1', 'PC2', 'PC3', 'PC4']]\n",
    "y_unseen = df_unseen['Category_column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b5721-a920-43ce-985d-6c08d48a03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up K-Fold cross-validation with 10 splits\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)\n",
    "\n",
    "#Initializing the XGBoost classifier \n",
    "XGB = xgb.XGBClassifier(objective=\"multi:softmax\", eval_metric=\"mlogloss\")\n",
    "\n",
    "#Extract the train and test from the k-fold\n",
    "train_index, test_index = next(k_fold.split(X, y))\n",
    "\n",
    "#Splitting the data into training and test sets \n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#Making surethe class labels start from 0, which is required for XGBoost with 'multi:softmax'\n",
    "y_train_fixed = y_train - y_train.min()\n",
    "y_test_fixed = y_test - y_test.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd75208-d687-4312-987c-0b022787bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a StandardScaler to normalize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fitting the scaler on the training data and applying the transformation\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc08f01-72ed-4213-9912-1f6526a72a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameter grid to search for the optimal hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [350],                  \n",
    "    'max_depth': [2, 3, 4],                 \n",
    "    'learning_rate': [0.01, 0.1],           \n",
    "    'subsample': [0.5, 0.7],               \n",
    "    'colsample_bytree': [0.9],             \n",
    "    'reg_alpha': [0.1, 1, 5, 10],           \n",
    "    'reg_lambda': [15]}\n",
    "\n",
    "#Initializing GridSearchCV to search over the parameter grid \n",
    "CV_XGB = GridSearchCV(\n",
    "    estimator=XGB,              \n",
    "    param_grid=param_grid,       \n",
    "    cv=k_fold,                 \n",
    "    scoring='f1_weighted',       \n",
    "    n_jobs=-1,               \n",
    "    verbose=1                   \n",
    ")\n",
    "\n",
    "#Fitting the grid search using the scaled training data \n",
    "CV_XGB.fit(X_train_scaled, y_train_fixed)\n",
    "\n",
    "#Printing the best hyperparameter combination \n",
    "print('Best Parameters: ', CV_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8881ba-ad29-43f0-b4b5-a43771f808b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the best XGBoost model found by GridSearchCV\n",
    "XGB_best = CV_XGB.best_estimator_\n",
    "\n",
    "#Saving the best hyperparameters found\n",
    "best_params = CV_XGB.best_params_\n",
    "\n",
    "#Making predictions on the scaled training data\n",
    "pred_train = XGB_best.predict(X_train_scaled)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "pred_test = XGB_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59434aca-c39d-4fb6-9ee5-ebfa09a025c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating performance metrics for the train data\n",
    "acc_train = accuracy_score(y_train_fixed, pred_train)\n",
    "precision_train = precision_score(y_train_fixed, pred_train, average='weighted')\n",
    "recall_train = recall_score(y_train_fixed, pred_train, average='weighted')\n",
    "f1_train = f1_score(y_train_fixed, pred_train, average='weighted')\n",
    "\n",
    "#Printing the evaluation metrics\n",
    "print(f'Train Accuracy: {acc_train:.4f}')\n",
    "print(f'Train Precision: {precision_train:.4f}')\n",
    "print(f'Train Recall: {recall_train:.4f}')\n",
    "print(f'Train F1 Score: {f1_train:.4f}')\n",
    "\n",
    "#Generating the confusion matrix \n",
    "cm = confusion_matrix(y_train_fixed, pred_train)\n",
    "\n",
    "#Creating readable class labels based on y_train\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_train_fixed)]\n",
    "\n",
    "#Visualizing the confusion matrix \n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08422fd1-6812-4d19-b59a-ef84ff704c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating performance metrics for the test data\n",
    "acc_test = accuracy_score(y_test_fixed, pred_test)\n",
    "precision_test = precision_score(y_test_fixed, pred_test, average='weighted')\n",
    "recall_test = recall_score(y_test_fixed, pred_test, average='weighted')\n",
    "f1_test = f1_score(y_test_fixed, pred_test, average='weighted')\n",
    "\n",
    "#Printing evaluation metrics\n",
    "print(f'Test Accuracy: {acc_test:.4f}')\n",
    "print(f'Test Precision: {precision_test:.4f}')\n",
    "print(f'Test Recall: {recall_test:.4f}')\n",
    "print(f'Test F1 Score: {f1_test:.4f}')\n",
    "\n",
    "#Generating confusion matrix to compare predicted VS true classes\n",
    "cm = confusion_matrix(y_test_fixed, pred_test)\n",
    "\n",
    "#Creating class labels based on y_test\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_test_fixed)]\n",
    "\n",
    "#Visualizing the confusion matrix as a heatmap\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de59ed-1686-429e-a882-d61f74e967ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting unseen target to start from 0 (same as training labels)\n",
    "y_unseen_fixed = y_unseen - y_train.min()\n",
    "\n",
    "#Scaling the unseen input features using the same scaler fitted on training data\n",
    "X_unseen_scaled = scaler.transform(X_unseen)\n",
    "\n",
    "\n",
    "#Making predictions on the unseen data\n",
    "y_pred_unseen = XGB_best.predict(X_unseen_scaled)\n",
    "\n",
    "#Calculating performance metrics for the unseen data\n",
    "acc_unseen = accuracy_score(y_unseen_fixed, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_fixed, y_pred_unseen, average='weighted', zero_division=0)\n",
    "recall_unseen = recall_score(y_unseen_fixed, y_pred_unseen, average='weighted', zero_division=0)\n",
    "f1_unseen = f1_score(y_unseen_fixed, y_pred_unseen, average='weighted', zero_division=0)\n",
    "\n",
    "#Printing the evaluation results for unseen data\n",
    "print(f'Unseen Accuracy: {acc_unseen:.4f}')\n",
    "print(f'Unseen Precision: {precision_unseen:.4f}')\n",
    "print(f'Unseen Recall: {recall_unseen:.4f}')\n",
    "print(f'Unseen F1 Score: {f1_unseen:.4f}')\n",
    "\n",
    "#Generate confusion matrix\n",
    "cm_unseen = confusion_matrix(y_unseen_fixed, y_pred_unseen)\n",
    "\n",
    "#Combining true and predicted labels to ensure all classes are included in the matrix labels\n",
    "all_labels = np.unique(np.concatenate((y_unseen_fixed, y_pred_unseen)))\n",
    "\n",
    "#Generating readable laberls\n",
    "class_labels = [f\"Class {i}\" for i in all_labels]\n",
    "\n",
    "#Visualizing the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_unseen, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbe8ed-7541-4d24-8420-200cba34dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, accuracy_train, precision_train, recall_train, f1_train,\n",
    "                accuracy_test, precision_test, recall_test, f1_test,\n",
    "                accuracy_unseen, precision_unseen, recall_unseen, f1_unseen,\n",
    "                filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs classification model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    # Creating DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params, \n",
    "        \"Accuracy_Train\": accuracy_train,\n",
    "        \"Precision_Train\": precision_train,\n",
    "        \"Recall_Train\": recall_train,\n",
    "        \"F1_Train\": f1_train,\n",
    "        \"Accuracy_Test\": accuracy_test,\n",
    "        \"Precision_Test\": precision_test,\n",
    "        \"Recall_Test\": recall_test,\n",
    "        \"F1_Test\": f1_test,\n",
    "        \"Accuracy_Unseen\": accuracy_unseen,\n",
    "        \"Precision_Unseen\": precision_unseen,\n",
    "        \"Recall_Unseen\": recall_unseen,\n",
    "        \"F1_Unseen\": f1_unseen}])\n",
    "\n",
    "    #Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Reading existing sheet \n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result  #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results, ensuring correct appending\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "        except PermissionError:\n",
    "            #Printing error to warn user\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "\n",
    "    #Printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train Accuracy={accuracy_train:.4f}, Test Accuracy={accuracy_test:.4f}, Unseen Accuracy={accuracy_unseen:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952e1d7-a7f6-4d20-9cf5-a17ade2d9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function to log results from XGBC model\n",
    "log_results(\n",
    "    model_name=\"XGB-class_PCA\",\n",
    "    params=best_params,\n",
    "\n",
    "    accuracy_train=acc_train,\n",
    "    precision_train=precision_train,\n",
    "    recall_train=recall_train,\n",
    "    f1_train=f1_train,\n",
    "\n",
    "    accuracy_test=acc_test,\n",
    "    precision_test=precision_test,\n",
    "    recall_test=recall_test,\n",
    "    f1_test=f1_test,\n",
    "\n",
    "    accuracy_unseen=acc_unseen,\n",
    "    precision_unseen=precision_unseen,\n",
    "    recall_unseen=recall_unseen,\n",
    "    f1_unseen=f1_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99928f5a-0153-4127-b736-86033fc2b267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57111fd-8068-4678-b8d5-38294d25263e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7813169-825f-4279-bc63-2f0d84fbff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
