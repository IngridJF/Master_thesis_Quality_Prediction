{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bc4ba-77a9-4474-809a-1705dd285f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing the XGBoost package\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b7fb0-cb4e-4617-86c6-2dcfad6ea78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24ef73-c26f-4c07-9de7-496ec8d146f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx')\n",
    "\n",
    "#dropping unwanted columns from the df\n",
    "df2=df.drop(columns = [\"Column 1\", \"Column 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b2108-a099-4ba0-b239-c2005e8cdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on ID\n",
    "df2[\"ID\"] = df2[\"ID\"].astype(str)  #Making sure its a string\n",
    "df_train = df2[df2[\"ID\"].isin([\"ID 1\", \"ID 2\", \"ID 3\"])]\n",
    "df_unseen = df2[df2[\"ID\"] == \"ID 4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765f73e-f87c-4265-b502-9f3a14c25100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing PCA input by defining the columns to include in the transformation\n",
    "columns = ['Column 1', 'Column 2', 'Column 3', 'Column 4', 'Column 5']\n",
    "\n",
    "#Extracting and copying the selected columns from the training dataframe\n",
    "data_train = df_train[columns].copy()\n",
    "\n",
    "#Encoding 'ID' \n",
    "data_train['ID'] = data_train['ID'].astype('Category_column').cat.codes\n",
    "\n",
    "#Initializing a standard scaler to normalize the data\n",
    "scaler_pca = StandardScaler()\n",
    "\n",
    "#Fitting the scaler on training data and apply the transformation\n",
    "data_train_scaled = scaler_pca.fit_transform(data_train)\n",
    "\n",
    "#Initializing PCA to reduce dimensionality to 4 PCs\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "#Fitting PCA on the scaled training data and transforming it\n",
    "pca_train = pca.fit_transform(data_train_scaled)\n",
    "\n",
    "#Adding the first 4 principal components (PC1 to PC4) as new columns to df_train\n",
    "for i in range(4):\n",
    "    df_train.loc[:, f'PC{i+1}'] = pca_train[:, i]\n",
    "\n",
    "\n",
    "#Applying the same PCA transformation to the unseen dataset \n",
    "\n",
    "#Extracting and copying the same columns from the unseen dataset\n",
    "data_unseen = df_unseen[columns].copy()\n",
    "\n",
    "#Encode 'ID' \n",
    "data_unseen['ID'] = data_unseen['ID'].astype('Category_column').cat.codes\n",
    "\n",
    "#Applying the same scaling \n",
    "data_unseen_scaled = scaler_pca.transform(data_unseen)\n",
    "\n",
    "#Applying the same PCA transformation\n",
    "pca_unseen = pca.transform(data_unseen_scaled)\n",
    "\n",
    "#Adding PC1 to PC4 as new columns to df_unseen\n",
    "for i in range(4):\n",
    "    df_unseen.loc[:, f'PC{i+1}'] = pca_unseen[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba93f5d-9144-4f6b-ad15-fe436274d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing inputs and target values\n",
    "X = df_train[['PC1', 'PC2', 'PC3', 'PC4']]\n",
    "y = df_train['Category_column']\n",
    "\n",
    "X_unseen = df_unseen[['PC1', 'PC2', 'PC3', 'PC4']]\n",
    "y_unseen = df_unseen['Category_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7280f7-e6aa-4ab6-ab24-c39a4bc7a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up K-Fold cross-validation with 10 splits\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)\n",
    "\n",
    "\n",
    "# Define a pipeline to standardize input features and implementing the XGB model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xg', xg.XGBRegressor(n_estimators=40, max_depth=7, random_state=40))])\n",
    "\n",
    "#Creating empty lists to store performance metrics for each fold\n",
    "mae_scores_train, mse_scores_train, r2_scores_train = [], [], []\n",
    "mae_scores_test, mse_scores_test, r2_scores_test = [], [], []\n",
    "\n",
    "#Performing the K-Fold cross-validation\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    #Splitting the data into training and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #Skipping this fold if the test set contains only one unique target value\n",
    "    if len(np.unique(y_test)) == 1:\n",
    "        print(\"Skipping this fold due to only one class in test set.\")\n",
    "        continue\n",
    "\n",
    "    #Training the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    #Making predictions on both training and test sets\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    #Evaluating the model on training data\n",
    "    mae_train = mean_absolute_error(y_train, pred_train)\n",
    "    mse_train = mean_squared_error(y_train, pred_train)\n",
    "    r2_train = r2_score(y_train, pred_train)\n",
    "\n",
    "    #Evaluating the model on test data\n",
    "    mae_test = mean_absolute_error(y_test, pred_test)\n",
    "    mse_test = mean_squared_error(y_test, pred_test)\n",
    "    r2_test = r2_score(y_test, pred_test)\n",
    "\n",
    "    #Saving metrics for this fold\n",
    "    mae_scores_train.append(mae_train)\n",
    "    mse_scores_train.append(mse_train)\n",
    "    r2_scores_train.append(r2_train)\n",
    "\n",
    "    mae_scores_test.append(mae_test)\n",
    "    mse_scores_test.append(mse_test)\n",
    "    r2_scores_test.append(r2_test)\n",
    "\n",
    "    #Printing performance metrics for this fold\n",
    "    print(f\"Train MAE: {mae_train:.6f}, MSE: {mse_train:.6f}, R²: {r2_train:.6f}\")\n",
    "    print(f\"Test  MAE: {mae_test:.6f}, MSE: {mse_test:.6f}, R²: {r2_test:.6f}\\n\")\n",
    "\n",
    "#Calculating and printing the average metrics for all folds\n",
    "print(\"\\nAverage results for all folds:\")\n",
    "print(f\"Train Mean MAE: {np.mean(mae_scores_train):.6f}, Test Mean MAE: {np.mean(mae_scores_test):.6f}\")\n",
    "print(f\"Train Mean MSE: {np.mean(mse_scores_train):.6f}, Test Mean MSE: {np.mean(mse_scores_test):.6f}\")\n",
    "print(f\"Train Mean R²: {np.mean(r2_scores_train):.6f}, Test Mean R²: {np.mean(r2_scores_test):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c46bf-abbe-46c5-92b6-1fda1cea7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a pipeline that standardizes features and applies the XGBoost regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),           \n",
    "    ('xg', xg.XGBRegressor(random_state=66)) \n",
    "])\n",
    "\n",
    "#Defining a hyperparameter grid to search for the optimal hyperparameters\n",
    "parameters_grid = {\n",
    "    'xg__n_estimators': [150, 250, 350],         \n",
    "    'xg__max_depth': [5, 7, 10],                \n",
    "    'xg__learning_rate': [0.05, 0.1],       \n",
    "    'xg__subsample': [0.8, 1],                  \n",
    "    'xg__colsample_bytree': [0.2, 0.3, 0.4],              \n",
    "    'xg__gamma': [1, 2],                           \n",
    "    'xg__reg_alpha': [0.5, 1, 5],                     \n",
    "    'xg__reg_lambda': [5, 10, 15],}\n",
    "\n",
    "#Setting up GridSearchCV to find the best combination of hyperparameters \n",
    "CV_XGB = GridSearchCV(\n",
    "    estimator=pipeline,              \n",
    "    param_grid=parameters_grid,      \n",
    "    cv=k_fold,                       \n",
    "    scoring='r2',                  \n",
    "    n_jobs=-1)\n",
    "\n",
    "#Fitting the grid search on the training data\n",
    "CV_XGB.fit(X_train, y_train)\n",
    "\n",
    "#Printing the best hyperparameter combination found\n",
    "print('Best parameters: ', CV_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f55fec-d9b9-41eb-9450-e220bfe849ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting best hyperparameters from the GridSearchCV results\n",
    "best_params = {key.replace(\"xg__\", \"\"): value for key, value in CV_XGB.best_params_.items()}\n",
    "\n",
    "#Saving the best pipeline from GridSearchCV\n",
    "best_pipeline = CV_XGB.best_estimator_\n",
    "\n",
    "#Making predictions on the training data\n",
    "y_train_pred = best_pipeline.predict(X_train)\n",
    "\n",
    "#Making predictions on the test data \n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "#Evaluating model performance on the training data\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)  \n",
    "r2_train = r2_score(y_train, y_train_pred)              \n",
    "\n",
    "#Evaluating model performance on the test data\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#Printing performance metrics\n",
    "print(f\"Train Mean Squared Error: {mse_train:.4f}\")\n",
    "print(f\"Train R-squared: {r2_train:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse_test:.4f}\")\n",
    "print(f\"Test R-squared: {r2_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00de1c0-212f-4d8d-a7b2-f9ee69853bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions on the unseen data\n",
    "y_unseen_pred = best_pipeline.predict(X_unseen)\n",
    "\n",
    "#Calculating performance metrics on the unseen data\n",
    "mse_unseen = mean_squared_error(y_unseen, y_unseen_pred)\n",
    "r2_unseen = r2_score(y_unseen, y_unseen_pred)\n",
    "\n",
    "#Printing evaluation results for the unseen dataset\n",
    "print(f'Unseen Mean Squared Error: {mse_unseen:.4f}')\n",
    "print(f'Unseen R-squared: {r2_unseen:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca34f2-2a78-4c82-8019-7f6c1958f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, mse_train, r2_train, mse_test, r2_test, mse_unseen, r2_unseen, filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    #Creating DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params, \n",
    "        \"MSE_Train\": mse_train,\n",
    "        \"R2_Train\": r2_train,\n",
    "        \"MSE_Test\": mse_test,\n",
    "        \"R2_Test\": r2_test,\n",
    "        \"MSE_Unseen\": mse_unseen,\n",
    "        \"R2_Unseen\": r2_unseen}])\n",
    "\n",
    "    #Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it properly before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Reading existing sheet \n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result   #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "        except PermissionError:\n",
    "            #Printing error to warn user\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "\n",
    "    #Printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train MSE={mse_train:.4f}, Test MSE={mse_test:.4f}, Unseen MSE={mse_unseen:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df27b6-c83c-4b35-9b70-dfb86a46a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function to log results from XGBR model\n",
    "log_results(\n",
    "    model_name=\"XGBoost_reg_PCA\",\n",
    "    params=best_params,\n",
    "    mse_train=mse_train, \n",
    "    r2_train=r2_train,\n",
    "    mse_test=mse_test,\n",
    "    r2_test=r2_test,\n",
    "    mse_unseen=mse_unseen,\n",
    "    r2_unseen=r2_unseen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d08e8-c2e5-41f3-9add-b1ef5c224a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5a14d-ba7e-4e6c-bd2d-a378548a84f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
