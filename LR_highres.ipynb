{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d84e7f-a8ba-43e8-9eb9-10b7d40a21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705cfa5-10bc-4823-a92f-2b555a9b1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx')\n",
    "\n",
    "#dropping unwanted columns from the df\n",
    "df2=df.drop(columns = [\"Column 1\", \"Column 2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfd7ac-df26-4e4b-8f74-26de5d42f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on ID\n",
    "df2[\"ID\"] = df2[\"ID\"].astype(str)  #Making sure its a string\n",
    "df_train = df2[df2[\"ID\"].isin([\"ID 1\", \"ID 2\", \"ID 3\"])]\n",
    "df_unseen = df2[df2[\"ID\"] == \"ID 4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa69071-cb3e-4a4c-af7c-07082e9c2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining which columns to keep av model input\n",
    "X = df_train.drop(columns = ['Column 1', 'Column 2', 'Column 3'])\n",
    "\n",
    "#defining which columns are the model outputs\n",
    "y = df_train['Column 4']\n",
    "\n",
    "\n",
    "#defining inputs and outputs for unseen dataset\n",
    "X_unseen = df_unseen.drop(columns = ['Column 1', 'Column 2', 'Column 3')\n",
    "y_unseen = df_unseen['Column 4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff863c5-d924-4ae5-aa55-33fabd0df838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a StandardScaler to normalize the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fitting the scaler on the training data only to avoid data leakage\n",
    "scaler.fit(X)\n",
    "\n",
    "#Transforming both training data and unseen data using the fitted scaler\n",
    "X_scaled = scaler.transform(X)\n",
    "X_unseen_scaled = scaler.transform(X_unseen)\n",
    "\n",
    "#Converting the scaled arrays back to DataFrames for easier column access later\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "X_unseen_scaled = pd.DataFrame(X_unseen_scaled, columns=X.columns)\n",
    "\n",
    "#Setting up K-Fold cross-validation (with 10 splits, shuffling, and a fixed random seed for reproducibility)\n",
    "k_fold = KFold(n_splits=10, random_state=66, shuffle=True)\n",
    "\n",
    "#Getting the number of splits\n",
    "k_fold.get_n_splits(X_scaled, y)\n",
    "\n",
    "#Initializing a logistic regression model with increased max_iter to ensure convergence\n",
    "LR = LogisticRegression(max_iter=4500)\n",
    "\n",
    "#Looping through the K-Fold splits with a for-loop\n",
    "for train_index, test_index in k_fold.split(X_scaled, y):\n",
    "    #Splitting the scaled data into training and testing sets for this fold\n",
    "    X_train, X_test = X_scaled.iloc[train_index], X_scaled.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccafb5-5d4e-42c4-be8b-6c7df81e9a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining the hyperparameter grid to search over.\n",
    "#'C' is the regularization strength\n",
    "#'solver' specifies the algorithm to use in the optimization problem\n",
    "#'penalty' is the norm used in the penalization\n",
    "param_grid = [\n",
    "    {'C': [0.1, 1, 10], 'solver': ['liblinear', 'saga', 'lbfgs'], 'penalty': ['l2', 'l1']},\n",
    "]\n",
    "\n",
    "#Seting up GridSearchCV to perform hyperparameter tuning on the logistic regression model.\n",
    "#cv=5: 5-fold cross-validation\n",
    "#scoring='f1_weighted': use weighted F1-score as evaluation metric\n",
    "#n_jobs=-1: use all available CPU cores for faster computation\n",
    "CV_LR = GridSearchCV(LR, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "#Fitting the model using the training data from one of the cross-validation splits\n",
    "CV_LR.fit(X_train, y_train)\n",
    "\n",
    "#Printing the best hyperparameter combination found by the grid search\n",
    "print('Best parameters: ', CV_LR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833480ba-6447-4ac0-aecd-8dcf025f92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the best hyperparameters found during GridSearchCV\n",
    "best_params = CV_LR.best_params_\n",
    "\n",
    "#Defining class weights to handle class imbalance\n",
    "class_weights = 'balanced'\n",
    "\n",
    "#Initializing a new Logistic Regression model using the best parameters from the grid search,\n",
    "#Setting a higher max_iter to ensure convergence and using class_weight='balanced'\n",
    "LR_best = LogisticRegression(**best_params, max_iter=3000, class_weight=class_weights)\n",
    "\n",
    "#Fitting the model on the training data\n",
    "LR_best.fit(X_train, y_train)\n",
    "\n",
    "#Predicting on the training set to evaluate performance\n",
    "pred_train = LR_best.predict(X_train)\n",
    "\n",
    "#Calculating performance metrics on the training data\n",
    "acc_train = accuracy_score(y_train, pred_train)                           #Overall accuracy\n",
    "precision_train = precision_score(y_train, pred_train, average='weighted')#Weighted precision\n",
    "recall_train = recall_score(y_train, pred_train, average='weighted')      #Weighted recall\n",
    "f1_train = f1_score(y_train, pred_train, average='weighted')              #Weighted F1-score\n",
    "\n",
    "#Printing metrics and evaluation details\n",
    "print(\"Accuracy:\", acc_train)\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, pred_train))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bc28c-05d0-4a5f-b164-ddac23a119bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions on the test data\n",
    "pred_test = LR_best.predict(X_test)\n",
    "\n",
    "#Calculating performance metrics on the test data\n",
    "acc_test = accuracy_score(y_test, pred_test)                              #Overall accuracy on test set\n",
    "precision_test = precision_score(y_test, pred_test, average='weighted')   #Weighted precision\n",
    "recall_test = recall_score(y_test, pred_test, average='weighted')         #Weighted recall\n",
    "f1_test = f1_score(y_test, pred_test, average='weighted')                 #Weighted F1-score\n",
    "\n",
    "#Printing evaluation results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_test))                     \n",
    "print(\"Classification Report:\\n\", classification_report(\n",
    "    y_test, pred_test, zero_division=1))                                   \n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_test))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c346b-ff42-44cc-b578-cbdc3b11cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions on the unseen data\n",
    "pred_unseen = LR_best.predict(X_unseen_scaled)\n",
    "\n",
    "#Calculating performance metrics on the unseen data\n",
    "acc_unseen = accuracy_score(y_unseen, pred_unseen)                            #Overall accuracy on the unseen data\n",
    "precision_unseen = precision_score(y_unseen, pred_unseen, average='weighted') #Weighted precision\n",
    "recall_unseen = recall_score(y_unseen, pred_unseen, average='weighted')       #Weighted recall\n",
    "f1_unseen = f1_score(y_unseen, pred_unseen, average='weighted')               #Weighted F1-score\n",
    "\n",
    "#Printingevaluation results\n",
    "print(\"Accuracy:\", accuracy_score(y_unseen, pred_unseen))\n",
    "print(\"Classification Report:\\n\", classification_report(y_unseen, pred_unseen, zero_division=1))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_unseen, pred_unseen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1341c94-2540-412b-9485-9837ddf1e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, accuracy_train, precision_train, recall_train, f1_train,\n",
    "                accuracy_test, precision_test, recall_test, f1_test,\n",
    "                accuracy_unseen, precision_unseen, recall_unseen, f1_unseen,\n",
    "                filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs classification model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    #Createing DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params,  #Storing hyperparameters\n",
    "        \"Accuracy_Train\": accuracy_train,\n",
    "        \"Precision_Train\": precision_train,\n",
    "        \"Recall_Train\": recall_train,\n",
    "        \"F1_Train\": f1_train,\n",
    "        \"Accuracy_Test\": accuracy_test,\n",
    "        \"Precision_Test\": precision_test,\n",
    "        \"Recall_Test\": recall_test,\n",
    "        \"F1_Test\": f1_test,\n",
    "        \"Accuracy_Unseen\": accuracy_unseen,\n",
    "        \"Precision_Unseen\": precision_unseen,\n",
    "        \"Recall_Unseen\": recall_unseen,\n",
    "        \"F1_Unseen\": f1_unseen\n",
    "    }])\n",
    "\n",
    "    #Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Reading existing sheet\n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result  #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results, ensuring correct appending\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "        #Printing error to warn user\n",
    "        except PermissionError:\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "    #printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train Accuracy={accuracy_train:.4f}, Test Accuracy={accuracy_test:.4f}, Unseen Accuracy={accuracy_unseen:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b1654-cd63-4721-88e4-bb06f307d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function to log results from LR model\n",
    "log_results(\n",
    "    model_name=\"LR\",\n",
    "    params=best_params,\n",
    "\n",
    "    accuracy_train=acc_train,\n",
    "    precision_train=precision_train,\n",
    "    recall_train=recall_train,\n",
    "    f1_train=f1_train,\n",
    "\n",
    "    accuracy_test=acc_test,\n",
    "    precision_test=precision_test,\n",
    "    recall_test=recall_test,\n",
    "    f1_test=f1_test,\n",
    "\n",
    "    accuracy_unseen=acc_unseen,\n",
    "    precision_unseen=precision_unseen,\n",
    "    recall_unseen=recall_unseen,\n",
    "    f1_unseen=f1_unseen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be252b3-b4e9-4cda-ac30-ba1e28240070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
