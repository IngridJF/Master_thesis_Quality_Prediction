{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bc4ba-77a9-4474-809a-1705dd285f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing XGBoost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b7fb0-cb4e-4617-86c6-2dcfad6ea78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,  precision_score, recall_score, f1_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24ef73-c26f-4c07-9de7-496ec8d146f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the excel-document into a df\n",
    "df = pd.read_excel('dataset_name.xlsx', Sheet_name = 'Sheet_name') \n",
    "\n",
    "#Choosing which columns to keep and saving them in a new df\n",
    "columns_to_keep = ['Column 1', 'Column 2', 'Column 3', 'Column 4', 'Column 5', '...']\n",
    "df2 = df[columns_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b2108-a099-4ba0-b239-c2005e8cdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting df into two based on hole ID\n",
    "df2.loc[:, 'Hole_id_stat_str'] = df2['Hole_id_stat'].astype(str)\n",
    "columns_unseen = ['209']\n",
    "df_train = df2[~df2['Hole_id_stat_str'].isin(columns_unseen)] \n",
    "df_unseen = df2[df2['Hole_id_stat_str'].isin(columns_unseen)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765f73e-f87c-4265-b502-9f3a14c25100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining which columns to keep av model input\n",
    "X = df_train.drop(columns = ['Column 1', 'Column 2', 'Column 3'])\n",
    "\n",
    "#Defining which columns are the model outputs\n",
    "y = df_train['Column 4']\n",
    "\n",
    "#Defining inputs and outputs for unseen dataset\n",
    "X_unseen = df_unseen.drop(columns = ['Column 1', 'Column 2', 'Column 3')\n",
    "y_unseen = df_unseen['Column 4'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7280f7-e6aa-4ab6-ab24-c39a4bc7a6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode labels before splitting, ensures all class labels are integers\n",
    "le = LabelEncoder()\n",
    "y_fixed = le.fit_transform(y) \n",
    "print(\"Encoded Classes:\", le.classes_)\n",
    "\n",
    "#Using K-Fold cross-validation to split the data into 5 folds\n",
    "k_fold = KFold(n_splits= 5, shuffle=True, random_state=66)\n",
    "\n",
    "#Creating lists to store splits\n",
    "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "#Performing KFold splitting \n",
    "for fold_num, (train_idx, test_idx) in enumerate(k_fold.split(X, y_fixed)):\n",
    "    print(f\"\\nProcessing Fold {fold_num + 1}\")\n",
    "\n",
    "    #Getting splits\n",
    "    X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "    y_train, y_test = y_fixed[train_idx], y_fixed[test_idx]\n",
    "\n",
    "    print(f\"Unique labels in y_train: {np.unique(y_train)}\")\n",
    "    print(f\"Unique labels in y_test: {np.unique(y_test)}\")\n",
    "\n",
    "    #Storing splits\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "#Using the first fold for training/testing\n",
    "X_train, X_test = X_train_list[0], X_test_list[0]\n",
    "y_train, y_test = y_train_list[0], y_test_list[0]\n",
    "\n",
    "#Printing results check\n",
    "print(f\"\\nFINAL CHECK:\")\n",
    "print(f\"Unique labels in FINAL y_train: {np.unique(y_train)}\")\n",
    "print(f\"Unique labels in FINAL y_test: {np.unique(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979258c4-2de2-4d4c-bf68-7bf0e1cea920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing input features using StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#Fitting the scaler on the training data and transforming it\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "#Using the same scaler to transform the test data \n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c46bf-abbe-46c5-92b6-1fda1cea7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the XGBoost classifier \n",
    "XGB = XGBClassifier(\n",
    "    objective=\"multi:softprob\",              \n",
    "    eval_metric=\"mlogloss\",                 \n",
    "    num_class=len(np.unique(y_train)),      \n",
    "    use_label_encoder=False,                 \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f55fec-d9b9-41eb-9450-e220bfe849ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameter grid \n",
    "param_grid = {\n",
    "    'n_estimators': [75],         \n",
    "    'max_depth': [5,10,15],              \n",
    "    'learning_rate': [0.2,0.5,0.7],        \n",
    "    'subsample': [0.4, 0.6],        \n",
    "    'colsample_bytree': [0.2, 0.4], \n",
    "    'reg_alpha': [0, 1, 5],         \n",
    "    'reg_lambda': [0, 1, 5]}\n",
    "\n",
    "#Setting up GridSearchCV to find the best combination of hyperparameters\n",
    "CV_XGB = GridSearchCV(\n",
    "    estimator=XGB,                 \n",
    "    param_grid=param_grid,        \n",
    "    cv=3,                          \n",
    "    scoring='accuracy',           \n",
    "    n_jobs=-1,                     \n",
    "    verbose=1)\n",
    "\n",
    "#Computing sample weights to balance class imbalance during training\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "#Fitting GridSearchCV to the training data with sample weights\n",
    "CV_XGB.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "\n",
    "#Printing the best hyperparameter combination found\n",
    "print('Best Parameters: ', CV_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00de1c0-212f-4d8d-a7b2-f9ee69853bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving the best XGBoost model found by GridSearchCV\n",
    "XGB_best = CV_XGB.best_estimator_\n",
    "\n",
    "#Making predictions on the training data using the best model\n",
    "pred_train = XGB_best.predict(X_train_scaled)\n",
    "\n",
    "#Making predictions on the test data using the best model\n",
    "pred_test = XGB_best.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c15b12-3557-4f59-b839-4138d8dddd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating performance metrics\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "precision_train = precision_score(y_train, pred_train, average='weighted', zero_division=1)\n",
    "recall_train = recall_score(y_train, pred_train, average='weighted')\n",
    "f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "\n",
    "#Printing all evaluation metrics with 4 decimals\n",
    "print(f'Accuracy: {acc_train:.4f}')\n",
    "print(f'Precision: {precision_train:.4f}')\n",
    "print(f'Recall: {recall_train:.4f}')\n",
    "print(f'F1 Score: {f1_train:.4f}')\n",
    "\n",
    "#Computing the confusion matrix \n",
    "cm = confusion_matrix(y_train, pred_train)\n",
    "\n",
    "#Creating readable labels for the axes\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_train)]\n",
    "\n",
    "#Visualizing the confusion matrix \n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,             \n",
    "    fmt=\"d\",                \n",
    "    cmap=\"Blues\",           \n",
    "    xticklabels=class_labels, \n",
    "    yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e73029-5b7f-482c-8a47-2dcd5a433dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating performance metrics for test data\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "precision_test = precision_score(y_test, pred_test, average=\"macro\", zero_division=1)\n",
    "recall_test = recall_score(y_test, pred_test, average=\"macro\", zero_division=1)\n",
    "f1_test = f1_score(y_test, pred_test, average='weighted')\n",
    "\n",
    "#Printing the evaluation metrics with 4 decimals\n",
    "print(f'Accuracy: {acc_test:.4f}')\n",
    "print(f'Precision: {precision_test:.4f}')\n",
    "print(f'Recall: {recall_test:.4f}')\n",
    "print(f'F1 Score: {f1_test:.4f}')\n",
    "\n",
    "#Creating a confusion matrix \n",
    "cm = confusion_matrix(y_test, pred_test)\n",
    "\n",
    "#Creating labels based on y_test\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_test)]\n",
    "\n",
    "#Plotting the confusion matrix\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,             \n",
    "    fmt=\"d\",                \n",
    "    cmap=\"Blues\",           \n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdce09-68c7-4211-b5ba-cc97f19bdd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacling the unseen inputs with the scaler fitted on the training data\n",
    "X_unseen_scaled = scaler.transform(X_unseen)\n",
    "y_pred_unseen = XGB_best.predict(X_unseen_scaled)\n",
    "y_unseen_encoded = le.transform(y_unseen)\n",
    "\n",
    "#calculating performance metrics on the unseen data\n",
    "acc_unseen = accuracy_score(y_unseen_encoded, y_pred_unseen)  \n",
    "precision_unseen = precision_score(y_unseen_encoded, y_pred_unseen, average='weighted', zero_division=0)  \n",
    "recall_unseen = recall_score(y_unseen_encoded, y_pred_unseen, average='weighted', zero_division=0)        \n",
    "f1_unseen = f1_score(y_unseen_encoded, y_pred_unseen, average='weighted', zero_division=0)                 \n",
    "\n",
    "#Printing the evaluation metrics\n",
    "print(f'Accuracy: {acc_unseen:.4f}')\n",
    "print(f'Precision: {precision_unseen:.4f}')\n",
    "print(f'Recall: {recall_unseen:.4f}')\n",
    "print(f'F1 Score: {f1_unseen:.4f}')\n",
    "\n",
    "\n",
    "#Generating the confusion matrix \n",
    "cm_unseen = confusion_matrix(y_unseen_encoded, y_pred_unseen)\n",
    "\n",
    "#Creating labels based on y_unseen\n",
    "class_labels = [f\"Class {i}\" for i in np.unique(y_unseen)]\n",
    "\n",
    "#Visualizing the confusion matrix \n",
    "sns.heatmap(\n",
    "    cm_unseen,\n",
    "    annot=True,                \n",
    "    fmt=\"d\",                  \n",
    "    cmap=\"Blues\",            \n",
    "    xticklabels=class_labels,  \n",
    "    yticklabels=class_labels   )\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca34f2-2a78-4c82-8019-7f6c1958f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to log model evaluation metrics and hyperparameters to an Excel-file of chosen name\n",
    "def log_results(model_name, params, accuracy_train, precision_train, recall_train, f1_train,\n",
    "                accuracy_test, precision_test, recall_test, f1_test,\n",
    "                accuracy_unseen, precision_unseen, recall_unseen, f1_unseen,\n",
    "                filename=\"Results.xlsx\"):\n",
    "    \"\"\" Logs classification model results in separate sheets within the same Excel file, ensuring appending works correctly. \"\"\"\n",
    "\n",
    "    #Creating DataFrame for this model run\n",
    "    result = pd.DataFrame([{\n",
    "        **params, \n",
    "        \"Accuracy_Train\": accuracy_train,\n",
    "        \"Precision_Train\": precision_train,\n",
    "        \"Recall_Train\": recall_train,\n",
    "        \"F1_Train\": f1_train,\n",
    "        \"Accuracy_Test\": accuracy_test,\n",
    "        \"Precision_Test\": precision_test,\n",
    "        \"Recall_Test\": recall_test,\n",
    "        \"F1_Test\": f1_test,\n",
    "        \"Accuracy_Unseen\": accuracy_unseen,\n",
    "        \"Precision_Unseen\": precision_unseen,\n",
    "        \"Recall_Unseen\": recall_unseen,\n",
    "        \"F1_Unseen\": f1_unseen\n",
    "    }])\n",
    "\n",
    "    #Introducing a short delay to avoid file conflicts if running in multiple notebooks\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Checking if the file exists\n",
    "    file_exists = os.path.exists(filename)\n",
    "\n",
    "    if not file_exists:\n",
    "        #If file doesn't exist, creating a new one\n",
    "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "            result.to_excel(writer, sheet_name=model_name, index=False)\n",
    "    else:\n",
    "        #If file exists, loading it properly before appending\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as writer:\n",
    "                #Reading existing sheet \n",
    "                try:\n",
    "                    existing_df = pd.read_excel(filename, sheet_name=model_name, engine=\"openpyxl\")\n",
    "                    df_combined = pd.concat([existing_df, result], ignore_index=True)\n",
    "                except (FileNotFoundError, ValueError):\n",
    "                    df_combined = result  #If sheet does not exist, creating it\n",
    "\n",
    "                #Saving results\n",
    "                df_combined.to_excel(writer, sheet_name=model_name, index=False)\n",
    "\n",
    "        except PermissionError:\n",
    "            #Printing error to warn user\n",
    "            print(f\"Error: Close the Excel file ({filename}) before running the script again.\")\n",
    "\n",
    "    #Printing a confirmation to ensure user results are logged\n",
    "    print(f\"Logged results for {model_name}: Train Accuracy={accuracy_train:.4f}, Test Accuracy={accuracy_test:.4f}, Unseen Accuracy={accuracy_unseen:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df27b6-c83c-4b35-9b70-dfb86a46a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function to log results from RF classifier model\n",
    "log_results(\n",
    "    model_name=\"XGBoost_class_stat\",\n",
    "    params=best_params,\n",
    "\n",
    "    accuracy_train=accuracy_score(y_train, pred_train),\n",
    "    precision_train=precision_score(y_train, pred_train, average='weighted', zero_division=0),\n",
    "    recall_train=recall_score(y_train, pred_train, average='weighted', zero_division=0),\n",
    "    f1_train=f1_score(y_train, pred_train, average='weighted', zero_division=0),\n",
    "\n",
    "    accuracy_test=accuracy_score(y_test, pred_test),\n",
    "    precision_test=precision_score(y_test, pred_test, average='weighted', zero_division=0),\n",
    "    recall_test=recall_score(y_test, pred_test, average='weighted', zero_division=0),\n",
    "    f1_test=f1_score(y_test, pred_test, average='weighted', zero_division=0),\n",
    "\n",
    "    accuracy_unseen=accuracy_score(y_unseen_encoded, y_pred_unseen),\n",
    "    precision_unseen=precision_score(y_unseen_encoded, y_pred_unseen, average='weighted', zero_division=0),\n",
    "    recall_unseen=recall_score(y_unseen_encoded, y_pred_unseen, average='weighted', zero_division=0),\n",
    "    f1_unseen=f1_score(y_unseen_encoded, y_pred_unseen, average='weighted', zero_division=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e86ec8-8233-499d-bece-80c898b9760f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98b437-7c27-44ec-8e7b-0255857ba0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
